{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T11:31:56.378895Z",
     "iopub.status.busy": "2025-04-03T11:31:56.378606Z",
     "iopub.status.idle": "2025-04-03T15:06:51.223620Z",
     "shell.execute_reply": "2025-04-03T15:06:51.222580Z",
     "shell.execute_reply.started": "2025-04-03T11:31:56.378873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaded: sequence length = 1024, d_input = 3, using 50.0% of training data\n",
      "Model architecture summary:\n",
      "Total parameters: 1938\n",
      "\n",
      "==================================================\n",
      "Training with zoh discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:16<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.0167, Train Acc=24.60%, Test Loss=1.8611, Test Acc=32.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=1.8784, Train Acc=30.41%, Test Loss=1.8347, Test Acc=32.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:14<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=1.8486, Train Acc=31.46%, Test Loss=1.8301, Test Acc=32.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=1.8236, Train Acc=32.82%, Test Loss=1.8126, Test Acc=33.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:13<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=1.8010, Train Acc=33.67%, Test Loss=1.7835, Test Acc=34.71%\n",
      "Best accuracy with zoh: 34.71%\n",
      "\n",
      "==================================================\n",
      "Training with bilinear discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:11<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.2791, Train Acc=19.75%, Test Loss=2.1944, Test Acc=22.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:09<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=2.0772, Train Acc=23.68%, Test Loss=2.2016, Test Acc=21.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:11<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=2.0072, Train Acc=25.54%, Test Loss=2.1096, Test Acc=24.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:14<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=1.9611, Train Acc=27.27%, Test Loss=2.0382, Test Acc=24.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:12<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=1.9210, Train Acc=29.00%, Test Loss=2.1967, Test Acc=20.36%\n",
      "Best accuracy with bilinear: 24.69%\n",
      "\n",
      "==================================================\n",
      "Training with generalized discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:11<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.2106, Train Acc=20.75%, Test Loss=2.0100, Test Acc=25.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=2.0007, Train Acc=25.75%, Test Loss=1.9443, Test Acc=28.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:11<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=1.9432, Train Acc=28.38%, Test Loss=1.9636, Test Acc=30.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:10<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=1.9202, Train Acc=29.12%, Test Loss=2.0019, Test Acc=27.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [12:10<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=1.9000, Train Acc=29.72%, Test Loss=1.8942, Test Acc=32.43%\n",
      "Best accuracy with generalized: 32.43%\n",
      "\n",
      "--- Final Results ---\n",
      "zoh: 34.71%\n",
      "bilinear: 24.69%\n",
      "generalized: 32.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reshape_to_sequence(x):\n",
    "    return x.reshape(3, 1024).permute(1, 0)  # (1024, 3)\n",
    "\n",
    "def load_sequential_cifar10(batch_size=128, fraction=0.2):\n",
    "    \"\"\"Load CIFAR-10 in sequential format with optional subset\"\"\"\n",
    "    # Transform to convert images to sequences\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(reshape_to_sequence)\n",
    "    ])\n",
    "\n",
    "    # Download and load datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    if fraction < 1.0:\n",
    "        train_size = len(trainset)\n",
    "        subset_size = int(train_size * fraction)\n",
    "        indices = torch.randperm(train_size)[:subset_size]\n",
    "        trainset = torch.utils.data.Subset(trainset, indices)\n",
    "\n",
    "    # Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return trainloader, testloader, 3  # 3 channels\n",
    "\n",
    "\n",
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, d_model, N=64, l_max=1024, discretization='zoh'):\n",
    "        \"\"\"\n",
    "        S4 layer implementation with support for different discretization schemes\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            N: State dimension\n",
    "            l_max: Maximum sequence length\n",
    "            discretization: Discretization scheme ('zoh', 'bilinear', or 'generalized')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.N = N\n",
    "        self.l_max = l_max\n",
    "        self.discretization = discretization\n",
    "        self.register_buffer('dummy', torch.zeros(1))\n",
    "\n",
    "        # Initialize SSM parameters\n",
    "        # A is the state matrix (N x N)\n",
    "        A = self._init_hippo_matrix()\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # Learnable parameters\n",
    "        # B is the input matrix (N x 1)\n",
    "        self.B = nn.Parameter(torch.randn(self.N, 1))\n",
    "        # C is the output matrix (1 x N)\n",
    "        self.C = nn.Parameter(torch.randn(1, self.N))\n",
    "        # D is the feedthrough term\n",
    "        self.D = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Timescale parameter (delta)\n",
    "        self.log_step = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Register buffers for discrete matrices\n",
    "        self.register_buffer('A_disc', torch.zeros(self.N, self.N))\n",
    "        self.register_buffer('B_disc', torch.zeros(self.N, 1))\n",
    "        self.register_buffer('k', torch.zeros(self.l_max))\n",
    "\n",
    "        # Initialize discrete-time matrices\n",
    "        self._setup_discretization()\n",
    "\n",
    "    def _init_hippo_matrix(self):\n",
    "        \"\"\"Initialize A using HiPPO Normal matrix\"\"\"\n",
    "        device = self.dummy.device\n",
    "        A = np.zeros((self.N, self.N))\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                if i < j:\n",
    "                    A[i, j] = 0\n",
    "                elif i == j:\n",
    "                    A[i, j] = -(2 * i + 1)\n",
    "                else:\n",
    "                    A[i, j] = -2 * np.sqrt((2 * i + 1) * (2 * j + 1))\n",
    "        return torch.from_numpy(A).float().to(device)\n",
    "\n",
    "    def _setup_discretization(self):\n",
    "        \"\"\"Set up discrete-time matrices based on the selected scheme\"\"\"\n",
    "        # Get step size\n",
    "        dt = torch.exp(self.log_step)\n",
    "        device = self.dummy.device\n",
    "\n",
    "        # Create identity matrix on the correct device\n",
    "        I = torch.eye(self.N, device=device)\n",
    "\n",
    "        if self.discretization == 'zoh':\n",
    "            # Zero-order hold discretization (standard)\n",
    "            self.A_disc = torch.matrix_exp(self.A * dt)\n",
    "            self.B_disc = torch.linalg.solve(\n",
    "                self.A,\n",
    "                (self.A_disc - I).matmul(self.B)\n",
    "            )\n",
    "\n",
    "        elif self.discretization == 'bilinear':\n",
    "            # Bilinear transform (Tustin's method)\n",
    "            left = torch.linalg.inv(I - dt/2 * self.A)\n",
    "            right = I + dt/2 * self.A\n",
    "            self.A_disc = left @ right\n",
    "            self.B_disc = left @ (dt * self.B)\n",
    "\n",
    "        elif self.discretization == 'generalized':\n",
    "            # Generalized bilinear transform\n",
    "            alpha = 0.5  # Can be made learnable\n",
    "            left = torch.linalg.inv(I - dt * (1-alpha) * self.A)\n",
    "            right = I + dt * alpha * self.A\n",
    "            self.A_disc = left @ right\n",
    "            self.B_disc = left @ (dt * self.B)\n",
    "\n",
    "        # Pre-compute kernel\n",
    "        self._compute_kernel()\n",
    "\n",
    "    def _compute_kernel(self):\n",
    "        \"\"\"Compute convolution kernel for fast inference\"\"\"\n",
    "        L = self.l_max\n",
    "        device = self.dummy.device\n",
    "\n",
    "        # Create tensor on the correct device\n",
    "        k = torch.zeros(L, dtype=torch.cfloat, device=device)\n",
    "\n",
    "        # Direct computation method\n",
    "        A_powers = self.A_disc.unsqueeze(0)  # [1, N, N]\n",
    "        for i in range(L):\n",
    "            if i > 0:\n",
    "                A_powers = A_powers @ self.A_disc\n",
    "            k[i] = (self.C @ A_powers.squeeze(0) @ self.B_disc).item()\n",
    "\n",
    "        # Store as real kernel for simplicity (imaginary part should be small)\n",
    "        self.k = torch.real(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass using convolution for efficiency\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        if self.dummy.device != device:\n",
    "            self.to(device)\n",
    "\n",
    "        # Recalculate discretization if parameters changed\n",
    "        if self.training:\n",
    "            self._setup_discretization()\n",
    "\n",
    "        # Process each feature dimension separately\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        # Causal convolution for each feature\n",
    "        for i in range(self.d_model):\n",
    "            # Extract feature i\n",
    "            xi = x[:, :, i].unsqueeze(1)  # [batch, 1, seq_len]\n",
    "\n",
    "            # Prepare kernel - ensure it's on the correct device\n",
    "            k_padded = nn.functional.pad(\n",
    "                self.k[:seq_len].flip(0).unsqueeze(0).unsqueeze(0),\n",
    "                (0, seq_len-1)\n",
    "            )\n",
    "\n",
    "            # Causal convolution\n",
    "            yi = nn.functional.conv1d(xi, k_padded, padding=seq_len-1)\n",
    "            yi = yi[:, :, :seq_len]\n",
    "\n",
    "            # Add D term (skip connection) and store result\n",
    "            out[:, :, i] = yi.squeeze(1) + self.D * x[:, :, i]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class S4Block(nn.Module):\n",
    "    \"\"\"S4 block with normalization, activation, and residual connection\"\"\"\n",
    "    def __init__(self, d_model, N=64, l_max=1024, discretization='zoh'):\n",
    "        super().__init__()\n",
    "        self.s4 = S4Layer(d_model, N, l_max, discretization)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-normalization\n",
    "        z = self.norm(x)\n",
    "\n",
    "        # Apply S4 layer\n",
    "        z = self.s4(z)\n",
    "\n",
    "        # Activation and dropout\n",
    "        z = self.activation(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + z\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "    \"\"\"Complete S4 model for classification\"\"\"\n",
    "    def __init__(self, d_input, d_model=64, n_layers=2, d_output=10, discretization='zoh'):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            S4Block(d_model, discretization=discretization)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification head\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input\n",
    "        x = self.encoder(x)  # [batch, seq_len, d_model]\n",
    "\n",
    "        # Apply S4 blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)  # [batch, d_model]\n",
    "\n",
    "        # Final classification\n",
    "        return self.decoder(x)  # [batch, d_output]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in tqdm(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, testloader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(testloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parameters - reduced for faster training\n",
    "    batch_size = 64\n",
    "    d_model = 64  \n",
    "    n_layers = 4 \n",
    "    epochs = 5   \n",
    "    fraction = 0.5  \n",
    "    discretization_schemes = ['zoh', 'bilinear', 'generalized']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data with subset for faster training\n",
    "    trainloader, testloader, d_input = load_sequential_cifar10(batch_size, fraction)\n",
    "    print(f\"Data loaded: sequence length = 1024, d_input = {d_input}, using {fraction*100}% of training data\")\n",
    "\n",
    "    # Model architecture summary\n",
    "    print(f\"Model architecture summary:\")\n",
    "    dummy_input = torch.zeros(1, 1024, d_input).to(device)\n",
    "    model = S4Model(d_input, d_model, n_layers, 10, 'zoh').to(device)\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Experiment with different discretization schemes\n",
    "    results = {}\n",
    "\n",
    "    for scheme in discretization_schemes:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with {scheme} discretization\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    \n",
    "        model = S4Model(\n",
    "            d_input=d_input,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "            d_output=10,\n",
    "            discretization=scheme\n",
    "        ).to(device)\n",
    "\n",
    "        # Setup optimizer with different learning rates\n",
    "        # S4 parameters need smaller learning rate with no weight decay\n",
    "        s4_params = []\n",
    "        other_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 's4.' in name and any(p in name for p in ['B', 'C', 'D', 'log_step']):\n",
    "                s4_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': s4_params, 'lr': 0.001, 'weight_decay': 0.0},\n",
    "            {'params': other_params, 'lr': 0.004, 'weight_decay': 0.01}\n",
    "        ])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "        # Training loop\n",
    "        best_acc = 0\n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "\n",
    "            # Evaluate\n",
    "            test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                  f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%\")\n",
    "\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stopping(test_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Store results\n",
    "        results[scheme] = best_acc\n",
    "        print(f\"Best accuracy with {scheme}: {best_acc:.2f}%\")\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    for scheme, acc in results.items():\n",
    "        print(f\"{scheme}: {acc:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
