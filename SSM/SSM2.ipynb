{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dqEhx-clHQ0"
   },
   "source": [
    "3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-XXAwTP04VB",
    "outputId": "f95275d7-47da-462a-8943-a59a77be220d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:28<00:00, 6.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: sequence length = 1024, d_input = 3, using 20.0% of training data\n",
      "Model architecture summary:\n",
      "Total parameters: 1422\n",
      "\n",
      "==================================================\n",
      "Training with zoh discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:18<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.2487, Train Acc=16.45%, Test Loss=2.1791, Test Acc=20.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=2.1458, Train Acc=21.46%, Test Loss=2.0884, Test Acc=23.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=2.0564, Train Acc=24.33%, Test Loss=2.0215, Test Acc=25.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=1.9773, Train Acc=26.98%, Test Loss=1.9990, Test Acc=24.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=1.9461, Train Acc=27.34%, Test Loss=1.9406, Test Acc=28.50%\n",
      "Best accuracy with zoh: 28.50%\n",
      "\n",
      "==================================================\n",
      "Training with bilinear discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.2385, Train Acc=17.47%, Test Loss=2.0953, Test Acc=22.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=2.0788, Train Acc=21.63%, Test Loss=2.0489, Test Acc=24.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=2.0549, Train Acc=23.15%, Test Loss=2.0489, Test Acc=22.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=2.0301, Train Acc=24.02%, Test Loss=2.0114, Test Acc=25.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:18<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=2.0078, Train Acc=24.83%, Test Loss=2.0375, Test Acc=22.92%\n",
      "Best accuracy with bilinear: 25.07%\n",
      "\n",
      "==================================================\n",
      "Training with generalized discretization\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:18<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss=2.2907, Train Acc=16.68%, Test Loss=2.1129, Test Acc=19.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss=2.0904, Train Acc=21.95%, Test Loss=2.0968, Test Acc=20.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:18<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss=2.0769, Train Acc=22.21%, Test Loss=2.0489, Test Acc=22.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:17<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss=2.0552, Train Acc=22.99%, Test Loss=2.0374, Test Acc=22.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [03:18<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss=2.0422, Train Acc=23.47%, Test Loss=2.0149, Test Acc=26.23%\n",
      "Best accuracy with generalized: 26.23%\n",
      "\n",
      "--- Final Results ---\n",
      "zoh: 28.50%\n",
      "bilinear: 25.07%\n",
      "generalized: 26.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_sequential_cifar10(batch_size=128, fraction=0.2):\n",
    "    \"\"\"Load CIFAR-10 in sequential format with optional subset\"\"\"\n",
    "    # Transform to convert images to sequences\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Flatten 32x32x3 image to sequence of length 1024 with 3 channels\n",
    "        transforms.Lambda(lambda x: x.reshape(3, 1024).permute(1, 0))  # (1024, 3)\n",
    "    ])\n",
    "\n",
    "    # Download and load datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "    if fraction < 1.0:\n",
    "        train_size = len(trainset)\n",
    "        subset_size = int(train_size * fraction)\n",
    "        indices = torch.randperm(train_size)[:subset_size]\n",
    "        trainset = torch.utils.data.Subset(trainset, indices)\n",
    "\n",
    "    # Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return trainloader, testloader, 3  # 3 channels\n",
    "\n",
    "\n",
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, d_model, N=64, l_max=1024, discretization='zoh'):\n",
    "        \"\"\"\n",
    "        S4 layer implementation with support for different discretization schemes\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            N: State dimension\n",
    "            l_max: Maximum sequence length\n",
    "            discretization: Discretization scheme ('zoh', 'bilinear', or 'generalized')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.N = N\n",
    "        self.l_max = l_max\n",
    "        self.discretization = discretization\n",
    "\n",
    "        self.register_buffer('dummy', torch.zeros(1))\n",
    "\n",
    "        # Initialize SSM parameters\n",
    "        # A is the state matrix (N x N)\n",
    "        A = self._init_hippo_matrix()\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # Learnable parameters\n",
    "        # B is the input matrix (N x 1)\n",
    "        self.B = nn.Parameter(torch.randn(self.N, 1))\n",
    "        # C is the output matrix (1 x N)\n",
    "        self.C = nn.Parameter(torch.randn(1, self.N))\n",
    "        # D is the feedthrough term\n",
    "        self.D = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Timescale parameter (delta)\n",
    "        self.log_step = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Register buffers for discrete matrices\n",
    "        self.register_buffer('A_disc', torch.zeros(self.N, self.N))\n",
    "        self.register_buffer('B_disc', torch.zeros(self.N, 1))\n",
    "        self.register_buffer('k', torch.zeros(self.l_max))\n",
    "\n",
    "        # Initialize discrete-time matrices\n",
    "        self._setup_discretization()\n",
    "\n",
    "    def _init_hippo_matrix(self):\n",
    "        \"\"\"Initialize A using HiPPO Normal matrix\"\"\"\n",
    "        device = self.dummy.device\n",
    "        A = np.zeros((self.N, self.N))\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                if i < j:\n",
    "                    A[i, j] = 0\n",
    "                elif i == j:\n",
    "                    A[i, j] = -(2 * i + 1)\n",
    "                else:\n",
    "                    A[i, j] = -2 * np.sqrt((2 * i + 1) * (2 * j + 1))\n",
    "        return torch.from_numpy(A).float().to(device)\n",
    "\n",
    "    def _setup_discretization(self):\n",
    "        \"\"\"Set up discrete-time matrices based on the selected scheme\"\"\"\n",
    "        # Get step size\n",
    "        dt = torch.exp(self.log_step)\n",
    "        device = self.dummy.device\n",
    "\n",
    "        # Create identity matrix on the correct device\n",
    "        I = torch.eye(self.N, device=device)\n",
    "\n",
    "        if self.discretization == 'zoh':\n",
    "            # Zero-order hold discretization (standard)\n",
    "            self.A_disc = torch.matrix_exp(self.A * dt)\n",
    "            self.B_disc = torch.linalg.solve(\n",
    "                self.A,\n",
    "                (self.A_disc - I).matmul(self.B)\n",
    "            )\n",
    "\n",
    "        elif self.discretization == 'bilinear':\n",
    "            # Bilinear transform (Tustin's method)\n",
    "            left = torch.linalg.inv(I - dt/2 * self.A)\n",
    "            right = I + dt/2 * self.A\n",
    "            self.A_disc = left @ right\n",
    "            self.B_disc = left @ (dt * self.B)\n",
    "\n",
    "        elif self.discretization == 'generalized':\n",
    "            # Generalized bilinear transform\n",
    "            alpha = 0.5  # Can be made learnable\n",
    "            left = torch.linalg.inv(I - dt * (1-alpha) * self.A)\n",
    "            right = I + dt * alpha * self.A\n",
    "            self.A_disc = left @ right\n",
    "            self.B_disc = left @ (dt * self.B)\n",
    "\n",
    "        # Pre-compute kernel\n",
    "        self._compute_kernel()\n",
    "\n",
    "    def _compute_kernel(self):\n",
    "        \"\"\"Compute convolution kernel for fast inference\"\"\"\n",
    "        L = self.l_max\n",
    "        device = self.dummy.device\n",
    "\n",
    "        # Create tensor on the correct device\n",
    "        k = torch.zeros(L, dtype=torch.cfloat, device=device)\n",
    "\n",
    "        # Direct computation method\n",
    "        A_powers = self.A_disc.unsqueeze(0)  # [1, N, N]\n",
    "        for i in range(L):\n",
    "            if i > 0:\n",
    "                A_powers = A_powers @ self.A_disc\n",
    "            k[i] = (self.C @ A_powers.squeeze(0) @ self.B_disc).item()\n",
    "\n",
    "        # Store as real kernel for simplicity (imaginary part should be small)\n",
    "        self.k = torch.real(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass using convolution for efficiency\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Ensure model parameters are on the same device as input\n",
    "        if self.dummy.device != device:\n",
    "            self.to(device)\n",
    "\n",
    "        # Recalculate discretization if parameters changed\n",
    "        if self.training:\n",
    "            self._setup_discretization()\n",
    "\n",
    "        # Process each feature dimension separately\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        # Causal convolution for each feature\n",
    "        for i in range(self.d_model):\n",
    "            # Extract feature i\n",
    "            xi = x[:, :, i].unsqueeze(1)  # [batch, 1, seq_len]\n",
    "\n",
    "            # Prepare kernel - ensure it's on the correct device\n",
    "            k_padded = nn.functional.pad(\n",
    "                self.k[:seq_len].flip(0).unsqueeze(0).unsqueeze(0),\n",
    "                (0, seq_len-1)\n",
    "            )\n",
    "\n",
    "            # Causal convolution\n",
    "            yi = nn.functional.conv1d(xi, k_padded, padding=seq_len-1)\n",
    "            yi = yi[:, :, :seq_len]\n",
    "\n",
    "            # Add D term (skip connection) and store result\n",
    "            out[:, :, i] = yi.squeeze(1) + self.D * x[:, :, i]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class S4Block(nn.Module):\n",
    "    \"\"\"S4 block with normalization, activation, and residual connection\"\"\"\n",
    "    def __init__(self, d_model, N=64, l_max=1024, discretization='zoh'):\n",
    "        super().__init__()\n",
    "        self.s4 = S4Layer(d_model, N, l_max, discretization)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-normalization\n",
    "        z = self.norm(x)\n",
    "\n",
    "        # Apply S4 layer\n",
    "        z = self.s4(z)\n",
    "\n",
    "        # Activation and dropout\n",
    "        z = self.activation(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # Residual connection\n",
    "        return x + z\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "    \"\"\"Complete S4 model for classification\"\"\"\n",
    "    def __init__(self, d_input, d_model=64, n_layers=2, d_output=10, discretization='zoh'):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            S4Block(d_model, discretization=discretization)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification head\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input\n",
    "        x = self.encoder(x)  # [batch, seq_len, d_model]\n",
    "\n",
    "        # Apply S4 blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)  # [batch, d_model]\n",
    "\n",
    "        # Final classification\n",
    "        return self.decoder(x)  # [batch, d_output]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in tqdm(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, testloader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(testloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parameters - reduced for faster training\n",
    "    batch_size = 64\n",
    "    d_model = 64  \n",
    "    n_layers = 2\n",
    "    epochs = 5   \n",
    "    fraction = 0.2  \n",
    "    discretization_schemes = ['zoh', 'bilinear', 'generalized']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data with subset for faster training\n",
    "    trainloader, testloader, d_input = load_sequential_cifar10(batch_size, fraction)\n",
    "    print(f\"Data loaded: sequence length = 1024, d_input = {d_input}, using {fraction*100}% of training data\")\n",
    "\n",
    "    # Model architecture summary\n",
    "    print(f\"Model architecture summary:\")\n",
    "    dummy_input = torch.zeros(1, 1024, d_input).to(device)\n",
    "    model = S4Model(d_input, d_model, n_layers, 10, 'zoh').to(device)\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Experiment with different discretization schemes\n",
    "    results = {}\n",
    "\n",
    "    for scheme in discretization_schemes:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with {scheme} discretization\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Create model and explicitly move to device\n",
    "        model = S4Model(\n",
    "            d_input=d_input,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "            d_output=10,\n",
    "            discretization=scheme\n",
    "        ).to(device)\n",
    "\n",
    "        # Setup optimizer with different learning rates\n",
    "        # S4 parameters need smaller learning rate with no weight decay\n",
    "        s4_params = []\n",
    "        other_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 's4.' in name and any(p in name for p in ['B', 'C', 'D', 'log_step']):\n",
    "                s4_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': s4_params, 'lr': 0.001, 'weight_decay': 0.0},\n",
    "            {'params': other_params, 'lr': 0.004, 'weight_decay': 0.01}\n",
    "        ])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "        # Training loop\n",
    "        best_acc = 0\n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "\n",
    "            # Evaluate\n",
    "            test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                  f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%\")\n",
    "\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stopping(test_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Store results\n",
    "        results[scheme] = best_acc\n",
    "        print(f\"Best accuracy with {scheme}: {best_acc:.2f}%\")\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    for scheme, acc in results.items():\n",
    "        print(f\"{scheme}: {acc:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsVbD_lnllCG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
